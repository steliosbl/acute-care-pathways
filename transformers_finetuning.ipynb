{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, warnings, dataclasses, itertools, argparse\n",
    "from pathlib import Path\n",
    "from functools import partial \n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy import stats as st\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport salford_datasets.salford, salford_datasets.salford_raw, transformer_experiment.utils.finetuning, transformer_experiment.salford_transformer_datasets\n",
    "\n",
    "from salford_datasets.salford import SalfordData, SalfordFeatures, SalfordPrettyPrint, SalfordCombinations\n",
    "\n",
    "from transformer_experiment.utils.finetuning import BERTModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notebook:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    DATA_DIR = Path('data/Salford/')\n",
    "    CACHE_DIR = Path('data/cache')\n",
    "    RE_DERIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SalfordTransformerDataset(torch.utils.data.Dataset):\n",
    "    _text: Iterable[str]\n",
    "    _labels: Iterable[str]\n",
    "    _avail_idx: Iterable[bool]\n",
    "    _text_tz: Iterable[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_SalfordData(cls, sal, model_uri, columns=SalfordCombinations.with_services):\n",
    "        _avail_idx = sal[columns].notna().any(axis=1)\n",
    "        _text = SalfordData(sal).tabular_to_text(columns)\n",
    "        _labels = sal.CriticalEvent.copy().astype(int).values\n",
    "\n",
    "        return cls(_text, _labels, _avail_idx).tokenise(model_uri)\n",
    "\n",
    "    def tokenise(self, model_uri):\n",
    "        tz =  AutoTokenizer.from_pretrained(model_uri)\n",
    "        tz_kwargs = dict(truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        self._text_tz = tz(list(self._text), **tz_kwargs)\n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return SalfordTransformerDataset(\n",
    "                _text = self._text[idx],\n",
    "                _labels = self._labels[idx],\n",
    "                _avail_idx = self._avail_idx.iloc[idx],\n",
    "                _text_tz = dict(\n",
    "                    input_ids = self._text_tz['input_ids'][idx],\n",
    "                    attention_mask = self._text_tz['attention_mask'][idx]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        return dict(\n",
    "            input_ids = self._text_tz['input_ids'][idx],\n",
    "            attention_mask = self._text_tz['attention_mask'][idx],\n",
    "            labels = self._labels[idx]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._text)\n",
    "\n",
    "    @property\n",
    "    def tensors(self):\n",
    "        return dict(\n",
    "            input_ids = torch.tensor(self._text_tz['input_ids']),\n",
    "            attention_mask = torch.tensor(self._text_tz['attention_mask'])\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuned Transformer\n",
    "\n",
    " - 4.1. Clinical notes on their own\n",
    " - 4.2. `with_services` on its own (text-ified)\n",
    " - 4.3. Expanded diagnoses on their own\n",
    " - 4.4. `with_services` and clinical notes\n",
    " - 4.5. All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_FEATURE_SETS = {\n",
    "    '41': SalfordFeatures.Text[:-2],\n",
    "    '42': SalfordCombinations.with_services,\n",
    "    '43': SalfordFeatures.Diagnoses,\n",
    "    '44': SalfordCombinations.with_services + SalfordFeatures.Text[:-2],\n",
    "    '45': SalfordCombinations.with_services + SalfordFeatures.Text[:-2] + SalfordFeatures.Diagnoses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformer_experiment.utils.finetuning import bert_finetuning_metrics\n",
    "from transformer_experiment.utils.finetuning import split_dict_into_batches, load_dict_to_device\n",
    "\n",
    "def finetune_note_transformer(sal_tz, model_uri, save_directory=\"bert-finetuned-notes_fake_delete\", batch_size=56):\n",
    "    bert_args = TrainingArguments(\n",
    "        Notebook.CACHE_DIR/save_directory,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='AP',\n",
    "        report_to='none',\n",
    "        optim=\"adamw_torch\",\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    bert_kwargs = dict(\n",
    "        num_labels=2, output_attentions=False, output_hidden_states=False, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    X_train, X_val = train_test_split(sal_tz, test_size=0.15, random_state=123, stratify=sal_tz._labels)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_uri, **bert_kwargs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        bert_args,\n",
    "        train_dataset=X_train,\n",
    "        eval_dataset=X_val,\n",
    "        compute_metrics=bert_finetuning_metrics\n",
    "    )\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        trainer.train()\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "def finetuned_inference(model, dataset):\n",
    "    with torch.no_grad():\n",
    "        X = split_dict_into_batches(dataset.tensors, 56)\n",
    "        y_pred_logit = torch.concat([\n",
    "            model(**load_dict_to_device(x)).logits for x in tqdm(X)\n",
    "        ])\n",
    "\n",
    "        y_pred_proba = F.softmax(y_pred_logit, dim=1)[:,1]\n",
    "        \n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenise_dataset(model_uri, re_derive=False, debug=False):\n",
    "    if re_derive:\n",
    "        logging.info('Deriving from raw dataset')\n",
    "        sal = SalfordData.from_raw(\n",
    "            pd.read_hdf(Notebook.DATA_DIR/'raw_v2.h5', 'table')\n",
    "        ).augment_derive_all().expand_icd10_definitions().sort_values('AdmissionDate')\n",
    "        sal.to_hdf(Notebook.DATA_DIR/'sal_processed_transformers.h5', 'table')\n",
    "    else:\n",
    "        logging.info('Loading processed dataset')\n",
    "        sal = SalfordData(pd.read_hdf(Notebook.DATA_DIR/'sal_processed_transformers.h5', 'table'))\n",
    "\n",
    "    if debug:\n",
    "        sal = sal.sample(100)\n",
    "        sal.loc[sal.sample(20).index, 'CriticalEvent'] = True\n",
    "    \n",
    "    sal_train, sal_test = train_test_split(SAL, test_size=0.33, shuffle=False)\n",
    "\n",
    "    logging.info('Tokenising feature set')\n",
    "    sal_bert_train = SalfordTransformerDataset.from_SalfordData(sal_train, model_uri, feature_set)\n",
    "    sal_bert_test = SalfordTransformerDataset.from_SalfordData(sal_test, model_uri, feature_set)\n",
    "\n",
    "    return sal_bert_train, sal_bert_test\n",
    "\n",
    "def load_tokenised_dataset_cached(bert_variant, experiment_num):\n",
    "    cache_filepath = Notebook.CACHE_DIR/f'sal_bert_{bert_variant}_{experiment_num}.bin'\n",
    "    if os.path.isfile(cache_filepath):\n",
    "        logging.info('Loading tokenised data from cache')\n",
    "        with open(cache_filepath, 'rb') as file:\n",
    "            sal_bert_train, sal_bert_test = pickle.load(file)\n",
    "    else:\n",
    "        sal_bert_train, sal_bert_test = tokenise_dataset(BERTModels[bert_variant])\n",
    "        with open(cache_filepath, 'wb') as file:\n",
    "            pickle.dump((sal_bert_train, sal_bert_test), file)\n",
    "    \n",
    "    return sal_bert_train, sal_bert_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_BioClinicalBert_41'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def get_checkpoint_directory(experiment_num='41', bert_variant='BioClinicalBert'):\n",
    "    model_directory = f'bert_{bert_variant}_{experiment_num}' \n",
    "    checkpoint_dir = [_ for _ in os.listdir(Notebook.CACHE_DIR/model_directory) if 'checkpoint-' in _]\n",
    "    checkpoint_dir = sorted(checkpoint_dir, key=lambda _: int(_.split('-')[1]))\n",
    "    checkpoint_dir = Notebook.CACHE_DIR/model_directory/(checkpoint_dir[-1])\n",
    "\n",
    "    return model_directory, checkpoint_dir\n",
    "\n",
    "def run_finetuning_4(experiment_num='41', bert_variant='BioClinicalBert', batch_size=56, debug=False):\n",
    "    feature_set = EXPERIMENT_FEATURE_SETS[experiment_num]\n",
    "    model_uri = BERTModels[bert_variant]\n",
    "\n",
    "    if debug:\n",
    "        model_directory = \"bert-finetuned-notes_fake_delete\"\n",
    "        sal_bert_train, _ = tokenise_dataset(model_uri, debug=True)\n",
    "    else:\n",
    "        model_directory, _ = get_checkpoint_directory(experiment_num, bert_variant)\n",
    "        sal_bert_train, sal_bert_test = load_tokenised_dataset_cached(bert_variant, experiment_num)\n",
    "\n",
    "    model = finetune_note_transformer(\n",
    "        sal_bert_train, model_uri, model_directory, batch_size\n",
    "    )\n",
    "\n",
    "    y_pred_proba = finetuned_inference(model, sal_bert_test)\n",
    "\n",
    "    with open(Notebook.CACHE_DIR/model_directory/'test_pred_proba.bin', 'wb') as file:\n",
    "        pickle.dump(y_pred_proba, file)\n",
    "\n",
    "\n",
    "def run_inference_4(experiment_num='41', bert_variant='BioClinicalBert', batch_size=56):\n",
    "    feature_set = EXPERIMENT_FEATURE_SETS[experiment_num]\n",
    "    model_uri = BERTModels[bert_variant]\n",
    "    \n",
    "    model_directory, checkpoint_dir = get_checkpoint_directory(experiment_num, bert_variant)\n",
    "\n",
    "    _, sal_bert_test = load_tokenised_dataset_cached(bert_variant, experiment_num)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir).to(Notebook.DEVICE).eval()\n",
    "\n",
    "    y_pred_proba = finetuned_inference(model, sal_bert_test)\n",
    "    \n",
    "    with open(Notebook.CACHE_DIR/model_directory/'test_pred_proba.bin', 'wb') as file:\n",
    "        pickle.dump(y_pred_proba, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_experiment.utils.finetuning import construct_parser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = construct_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run_finetuning_4(args.experiment, args.model, args.batch_size, args.debug)\n",
    "    #run_inference_4(args.experiment, args.model, args.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2104865e4d02caf357b4f17570cecf3aad1d3e04a9c3efec371f5583b6707d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
